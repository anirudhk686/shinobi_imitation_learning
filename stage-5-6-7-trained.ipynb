{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre03/project/6003287/ani686/env/lib/python3.6/site-packages/nilearn/datasets/__init__.py:90: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn import input_data, plotting,datasets\n",
    "import pickle\n",
    "import sys\n",
    "from models import get_models\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut,KFold\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETER CELL \n",
    "\n",
    "model_subject = subject\n",
    "load_epoch = 3000\n",
    "model_num = 1\n",
    "    \n",
    "LOAD_PATH = '../models/'+'sub-'+str(model_subject)+'-model-'+str(model_num)+'/'+str(load_epoch)+'.torch'\n",
    "load_model = True\n",
    "\n",
    "\n",
    "frame_skip = 5\n",
    "sequence_len = int(90/frame_skip)\n",
    "batch_size = 1\n",
    "\n",
    "h5_filename = '../data/bw-'+'sub-'+str(subject)+'-len-'+str(sequence_len)+'.h5'#data file to train\n",
    "df_filepath_stage1 = '../temp_files/'+'sub-'+str(subject)+'-stage-1-df.pkl'\n",
    "runshape_file_path = '../temp_files/'+'sub-'+str(subject)+'-stage-2-runshapes-'+str(sequence_len)+'.pkl'\n",
    "fmri_file_path = '../temp_files/'+'sub-'+str(subject)+'-stage-3-parcel-confounds9-nohigh.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subject==1:\n",
    "    exclude_runs = []\n",
    "if subject==2:\n",
    "    exclude_runs = []   \n",
    "if subject==4:\n",
    "    exclude_runs = []\n",
    "if subject==6:\n",
    "    exclude_runs = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NiftiLabelsMasker(high_pass=0.01,\n",
       "                  labels_img='/home/ani686/nilearn_data/basc_multiscale_2015/template_cambridge_basc_multiscale_nii_sym/template_cambridge_basc_multiscale_sym_scale444.nii.gz',\n",
       "                  memory='nilearn_cache', smoothing_fwhm=8, standardize=True,\n",
       "                  t_r=1.49)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Concatdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)\n",
    "    \n",
    "dataset = datasets.fetch_atlas_basc_multiscale_2015()\n",
    "atlas_filename = dataset.scale444\n",
    "masker = input_data.NiftiLabelsMasker(labels_img=atlas_filename,high_pass=0.01 ,standardize=True,t_r=1.49,smoothing_fwhm=8,memory='nilearn_cache')\n",
    "masker.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame data loaded\n",
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(h5_filename,'r')\n",
    "data = f['state']\n",
    "label = f['action']\n",
    "sess = f['session']\n",
    "run = f['run']\n",
    "data_all = np.array(data)\n",
    "label_all = np.array(label)\n",
    "sess_all = np.array(sess)\n",
    "run_all = np.array(run)\n",
    "main_dataset = Concatdataset(data_all,label_all,sess_all,run_all)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('frame data loaded')\n",
    "\n",
    "model = get_models(model_num,sequence_len)\n",
    "model = model.to(device)\n",
    "if load_model:\n",
    "    saved_state = torch.load(LOAD_PATH ,map_location=device)\n",
    "    model.load_state_dict(saved_state)\n",
    "    print('Loaded model',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_5(model,main_dataset):\n",
    "    if True:\n",
    "    \n",
    "        test_loader = DataLoader(dataset=main_dataset, shuffle=False, batch_size=1)\n",
    "        \n",
    "        conv1 = []\n",
    "        conv2 = []\n",
    "        conv3 = []\n",
    "        conv4 = []\n",
    "        lstm = []\n",
    "        last = []\n",
    "        label = []\n",
    "        sess_all = []\n",
    "        run_all = []\n",
    "\n",
    "\n",
    "        if True:\n",
    "            test_accuracy=0.0\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batchid,traindata in enumerate(test_loader):\n",
    "                    data,labels,sess,run = traindata\n",
    "                    \n",
    "                    labels = labels.reshape(-1).to(device=device)\n",
    "                   \n",
    "                    data = data.float().to(device=device)\n",
    "                    out,[conv1_vecs,conv2_vecs,conv3_vecs,conv4_vecs,lstm_vecs,last_vecs],_ = model(data)\n",
    "                    \n",
    "                    \n",
    "                    conv1_vecs = conv1_vecs.detach().cpu().numpy()\n",
    "                    conv2_vecs = conv2_vecs.detach().cpu().numpy()\n",
    "                    conv3_vecs = conv3_vecs.detach().cpu().numpy()\n",
    "                    conv4_vecs = conv4_vecs.detach().cpu().numpy()\n",
    "                    lstm_vecs = lstm_vecs.detach().cpu().numpy()\n",
    "                    last_vecs = last_vecs.detach().cpu().numpy()\n",
    "                    label_vecs = labels.detach().cpu().numpy()\n",
    "                    \n",
    "                    \n",
    "                    conv1.append(conv1_vecs)\n",
    "                    conv2.append(conv2_vecs)\n",
    "                    conv3.append(conv3_vecs)\n",
    "                    conv4.append(conv4_vecs)\n",
    "                    lstm.append(lstm_vecs)\n",
    "                    last.append(last_vecs)\n",
    "                    label.append(label_vecs)\n",
    "                    sess_all.append(sess)\n",
    "                    run_all.append(run)\n",
    "                    \n",
    "                    \n",
    "                    probs = torch.exp(out)                \n",
    "                    top_p, top_class = probs.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            \n",
    "            test_accuracy=test_accuracy/len(test_loader)\n",
    "\n",
    "            print(\"Accuracy: %.3f\"%(test_accuracy))\n",
    "            all_dict = {'conv1':conv1,'conv2':conv2,'conv3':conv3,'conv4':conv4,'lstm':lstm,'last':last,'label':label,'session':sess_all,'run':run_all}\n",
    "            return all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_6(dl_activations):\n",
    "    if True:\n",
    "        conv1 = np.array(dl_activations['conv1']) \n",
    "        conv2 = np.array(dl_activations['conv2']) \n",
    "        conv3 = np.array(dl_activations['conv3']) \n",
    "        conv4 = np.array(dl_activations['conv4']) \n",
    "        lstm = np.array(dl_activations['lstm']) \n",
    "        last = np.array(dl_activations['last'])\n",
    "        label = np.array(dl_activations['label'])\n",
    "        session = np.array(dl_activations['session'])\n",
    "        run = np.array(dl_activations['run'])\n",
    "        \n",
    "        with open(runshape_file_path, 'rb') as f:\n",
    "            test_shapes = pickle.load(f)\n",
    "        dl_all = np.arange(conv3.shape[0])\n",
    "        \n",
    "        #split into runs\n",
    "        dl_final = []\n",
    "        start_idx = 0\n",
    "        stop_idx = 0\n",
    "        for i in range(len(test_shapes)):\n",
    "            start_idx = stop_idx\n",
    "            stop_idx = start_idx+test_shapes[i]\n",
    "            dl_final.append(dl_all[start_idx:stop_idx])\n",
    "        \n",
    "        tsum = 0\n",
    "        for i in range(len(dl_final)):\n",
    "            tsum = tsum + dl_final[i].shape[0]\n",
    "        #shape match check\n",
    "        if not(sum(test_shapes)==dl_all.shape[0]==tsum):\n",
    "            print('not pass 1')\n",
    "        \n",
    "        with open(fmri_file_path, 'rb') as f:\n",
    "            fmri_all = pickle.load(f)\n",
    "        df = pd.read_pickle(df_filepath_stage1)\n",
    "        \n",
    "        fmri_all2 = []\n",
    "        for i in range(len(df)):\n",
    "            onset_index = int(df['onset'][i]/1.49)\n",
    "            fmri_all2.append(fmri_all[i][onset_index:])\n",
    "        for i in range(len(fmri_all2)):\n",
    "            if(fmri_all2[i].shape[0]*1.49<df['duration'][i]):\n",
    "                print('not pass 2',i)\n",
    "                \n",
    "        fmri_all3 = []\n",
    "        for i in range(len(df)):\n",
    "            stop_index = int(df['duration'][i]/1.49)\n",
    "            fmri_all3.append(fmri_all2[i][:stop_index]) \n",
    "        for i in range(len(fmri_all3)):\n",
    "            diff = fmri_all3[i].shape[0]*1.49 - df['duration'][i]\n",
    "            if not(0>diff and diff>-1.49):\n",
    "                print('Not pass 3',i,diff)\n",
    "          \n",
    "        for i in range(len(df)):\n",
    "            if(fmri_all3[i].shape[0]-dl_final[i].shape[0]<0):\n",
    "                print('not pass 4',i)\n",
    "          \n",
    "                \n",
    "        #the dl is always small above\n",
    "        for i in range(len(df)):\n",
    "            fmri_all3[i]= fmri_all3[i][:dl_final[i].shape[0]]\n",
    "        for i in range(len(df)):\n",
    "            if not(fmri_all3[i].shape[0]==dl_final[i].shape[0]):\n",
    "                print('not pass 5',i)\n",
    "\n",
    "        lag_range = [4] # each lag = 1.5 secs\n",
    "        # find out common len for all lags\n",
    "        # largest lag will have shortest array size\n",
    "        common_length = np.zeros(len(df))\n",
    "        for i in range(len(df)):\n",
    "            common_length[i] = fmri_all3[i].shape[0]-max(lag_range)\n",
    "\n",
    "        for lag in lag_range:\n",
    "            #below note dl_final deals with indices\n",
    "            dl_final_all = []\n",
    "            fmri_final_all = []\n",
    "            for i in range(len(df)):\n",
    "                \n",
    "                dl_final_all.append(dl_final[i][:(fmri_all3[i].shape[0]-lag)])\n",
    "                fmri_final_all.append(fmri_all3[i][lag:])    \n",
    "\n",
    "            #for same size vectors for all considered lags\n",
    "            for i in range(len(df)):\n",
    "                dl_final_all[i] = dl_final_all[i][int(-common_length[i]):]\n",
    "                fmri_final_all[i] = fmri_final_all[i][int(-common_length[i]):]\n",
    "                \n",
    "            for i in range(len(df)):\n",
    "                if not(dl_final_all[i].shape[0]==fmri_final_all[i].shape[0]):\n",
    "                    print('not pass 6',i)\n",
    "                    \n",
    "            for i in range(len(df)):\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(fmri_final_all[i])\n",
    "                fmri_final_all[i] = scaler.transform(fmri_final_all[i])\n",
    "            \n",
    "            \n",
    "            new_dl_final = []\n",
    "            new_fmri_final = []\n",
    "            for i in range(len(df)):\n",
    "                if i not in exclude_runs:\n",
    "                    new_dl_final.append(dl_final_all[i])\n",
    "                    new_fmri_final.append(fmri_final_all[i])\n",
    "                \n",
    "            \n",
    "            \n",
    "            dl = np.concatenate(new_dl_final[:],axis=0)\n",
    "            fmri = np.concatenate(new_fmri_final[:],axis=0)\n",
    "            tconv1 = conv1[dl]\n",
    "            tconv2 = conv2[dl]\n",
    "            tconv3 = conv3[dl]\n",
    "            tconv4 = conv4[dl]\n",
    "            tlstm = lstm[dl]\n",
    "            tlast = last[dl]\n",
    "            tlabel = label[dl]\n",
    "            tsession = session[dl]\n",
    "            trun = run[dl]\n",
    "\n",
    "        all_dict = {'conv1':tconv1,'conv2':tconv2,'conv3':tconv3,'conv4':tconv4,'lstm':tlstm,'last':tlast,'label':tlabel,'fmri':fmri,'session':tsession,'run':trun}\n",
    "        return all_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.958\n"
     ]
    }
   ],
   "source": [
    "dl_activations = stage_5(model,main_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_fmri = stage_6(dl_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9769308020673267\n",
      "0.8670185\n",
      "0.9963820817489253\n",
      "0.9135489\n",
      "0.9793761242091712\n",
      "0.8104419\n",
      "0.9999999999999999\n",
      "0.94022024\n",
      "1.0\n",
      "0.8997724\n",
      "1.0\n",
      "0.9958921\n"
     ]
    }
   ],
   "source": [
    "conv1 = dl_fmri['conv1']\n",
    "conv2 = dl_fmri['conv2']\n",
    "conv3 = dl_fmri['conv3']\n",
    "conv4 = dl_fmri['conv4']\n",
    "lstm = dl_fmri['lstm']\n",
    "last = dl_fmri['last']\n",
    "labels = dl_fmri['label']\n",
    "fmri = dl_fmri['fmri']\n",
    "\n",
    "\n",
    "def transform_layer_activations2(layer,pca1=800):\n",
    "    layer = layer.reshape((-1,layer.shape[-1]))\n",
    "    pca = PCA(n_components=pca1)\n",
    "    pca.fit(layer)\n",
    "    print(pca.explained_variance_ratio_.sum())\n",
    "    layer = pca.transform(layer)\n",
    "    layer = layer.reshape((-1,sequence_len,layer.shape[-1]))\n",
    "    layer = layer.reshape(layer.shape[0],-1)\n",
    "    pca = PCA(n_components=300)\n",
    "    \n",
    "    pca.fit(layer)\n",
    "    print(pca.explained_variance_ratio_.sum())\n",
    "    layer = pca.transform(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\n",
    "conv1pca2 = transform_layer_activations2(conv1)\n",
    "conv2pca2 = transform_layer_activations2(conv2)\n",
    "conv3pca2 = transform_layer_activations2(conv3)\n",
    "conv4pca2 = transform_layer_activations2(conv4,pca1=200)\n",
    "lstmpca2 = transform_layer_activations2(lstm,pca1=200)\n",
    "lastpca2 = transform_layer_activations2(last,pca1=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68 10  0  0]\n"
     ]
    }
   ],
   "source": [
    "dl_fin_temp = np.concatenate((conv1pca2,conv2pca2,conv4pca2,conv3pca2,lstmpca2),axis=1)\n",
    "\n",
    "\n",
    "dl_fin = dl_fin_temp\n",
    "alpha = 0.5\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "count = 0\n",
    "all_test=[]\n",
    "for train_index, test_index in logo.split(dl_fin, fmri, dl_fmri['session']):\n",
    "    count = count +1\n",
    "    clf = Ridge(alpha=alpha,normalize=True)\n",
    "\n",
    "    clf.fit(dl_fin[train_index], fmri[train_index])\n",
    "    train_predict = clf.predict(dl_fin[train_index])\n",
    "    train_true = fmri[train_index]\n",
    "    r2_train = r2_score(train_true,train_predict,multioutput='raw_values').clip(min=0)\n",
    "\n",
    "    logo2 =  LeaveOneGroupOut()\n",
    "    \n",
    "    for run_train_index, run_test_index in logo2.split(dl_fin[test_index],fmri[test_index], dl_fmri['run'][test_index]):\n",
    "        test_predict = clf.predict(dl_fin[test_index[run_test_index]])\n",
    "        test_true = fmri[test_index[run_test_index]]\n",
    "        r2_test = r2_score(test_true,test_predict,multioutput='raw_values').clip(min=0)\n",
    "        all_test.append(r2_test)\n",
    "            \n",
    "all_test = np.array(all_test[:])\n",
    "mean_r2  = all_test.mean(axis=0)\n",
    "            \n",
    "print(np.histogram(mean_r2, bins=[0.1, 0.2, 0.3,0.4,0.5])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40  4  0  0]\n"
     ]
    }
   ],
   "source": [
    "dl_fin_temp = np.concatenate((conv1pca2,conv2pca2),axis=1)\n",
    "number = 91\n",
    "\n",
    "dl_fin = dl_fin_temp\n",
    "alpha = 0.2\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "count = 0\n",
    "all_test=[]\n",
    "for train_index, test_index in logo.split(dl_fin, fmri, dl_fmri['session']):\n",
    "    count = count +1\n",
    "    clf = Ridge(alpha=alpha,normalize=True)\n",
    "\n",
    "    clf.fit(dl_fin[train_index], fmri[train_index])\n",
    "    train_predict = clf.predict(dl_fin[train_index])\n",
    "    train_true = fmri[train_index]\n",
    "    r2_train = r2_score(train_true,train_predict,multioutput='raw_values').clip(min=0)\n",
    "\n",
    "    logo2 =  LeaveOneGroupOut()\n",
    "    \n",
    "    for run_train_index, run_test_index in logo2.split(dl_fin[test_index],fmri[test_index], dl_fmri['run'][test_index]):\n",
    "        test_predict = clf.predict(dl_fin[test_index[run_test_index]])\n",
    "        test_true = fmri[test_index[run_test_index]]\n",
    "        r2_test = r2_score(test_true,test_predict,multioutput='raw_values').clip(min=0)\n",
    "        all_test.append(r2_test)\n",
    "            \n",
    "all_test = np.array(all_test[:])\n",
    "mean_r2  = all_test.mean(axis=0)\n",
    "            \n",
    "print(np.histogram(mean_r2, bins=[0.1, 0.2, 0.3,0.4,0.5])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('results/main-'+str(subject)+'-'+str(number)+'.npy',all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57  3  0  0]\n"
     ]
    }
   ],
   "source": [
    "dl_fin_temp = np.concatenate((conv4pca2,lstmpca2),axis=1)\n",
    "\n",
    "number = 92\n",
    "dl_fin = dl_fin_temp\n",
    "alpha = 0.2\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "count = 0\n",
    "all_test=[]\n",
    "for train_index, test_index in logo.split(dl_fin, fmri, dl_fmri['session']):\n",
    "    count = count +1\n",
    "    clf = Ridge(alpha=alpha,normalize=True)\n",
    "\n",
    "    clf.fit(dl_fin[train_index], fmri[train_index])\n",
    "    train_predict = clf.predict(dl_fin[train_index])\n",
    "    train_true = fmri[train_index]\n",
    "    r2_train = r2_score(train_true,train_predict,multioutput='raw_values').clip(min=0)\n",
    "\n",
    "    logo2 =  LeaveOneGroupOut()\n",
    "    \n",
    "    for run_train_index, run_test_index in logo2.split(dl_fin[test_index],fmri[test_index], dl_fmri['run'][test_index]):\n",
    "        test_predict = clf.predict(dl_fin[test_index[run_test_index]])\n",
    "        test_true = fmri[test_index[run_test_index]]\n",
    "        r2_test = r2_score(test_true,test_predict,multioutput='raw_values').clip(min=0)\n",
    "        all_test.append(r2_test)\n",
    "            \n",
    "all_test = np.array(all_test[:])\n",
    "mean_r2  = all_test.mean(axis=0)\n",
    "            \n",
    "print(np.histogram(mean_r2, bins=[0.1, 0.2, 0.3,0.4,0.5])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('results/main-'+str(subject)+'-'+str(number)+'.npy',all_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
