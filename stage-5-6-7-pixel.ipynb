{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre03/project/6003287/ani686/env/lib/python3.6/site-packages/nilearn/datasets/__init__.py:90: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn import input_data, plotting,datasets\n",
    "import pickle\n",
    "import sys\n",
    "from models import get_models\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut,KFold\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETER CELL \n",
    "model_subject = subject\n",
    "\n",
    "load_epoch = 3000\n",
    "model_num = 1\n",
    "    \n",
    "LOAD_PATH = '../models/'+'sub-'+str(model_subject)+'-model-'+str(model_num)+'/'+str(load_epoch)+'.torch'\n",
    "load_model = True\n",
    "\n",
    "frame_skip = 5\n",
    "sequence_len = int(90/frame_skip)\n",
    "batch_size = 1\n",
    "\n",
    "h5_filename = '../data/bw-'+'sub-'+str(subject)+'-len-'+str(sequence_len)+'.h5'#data file to train\n",
    "df_filepath_stage1 = '../temp_files/'+'sub-'+str(subject)+'-stage-1-df.pkl'\n",
    "runshape_file_path = '../temp_files/'+'sub-'+str(subject)+'-stage-2-runshapes-'+str(sequence_len)+'.pkl'\n",
    "fmri_file_path = '../temp_files/'+'sub-'+str(subject)+'-stage-3-parcel-confounds9-nohigh.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subject==1:\n",
    "    exclude_runs = []\n",
    "if subject==2:\n",
    "    exclude_runs = []   \n",
    "if subject==4:\n",
    "    exclude_runs = []\n",
    "if subject==6:\n",
    "    exclude_runs = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NiftiLabelsMasker(high_pass=0.01,\n",
       "                  labels_img='/home/ani686/nilearn_data/basc_multiscale_2015/template_cambridge_basc_multiscale_nii_sym/template_cambridge_basc_multiscale_sym_scale444.nii.gz',\n",
       "                  memory='nilearn_cache', smoothing_fwhm=8, standardize=True,\n",
       "                  t_r=1.49)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Concatdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)\n",
    "    \n",
    "dataset = datasets.fetch_atlas_basc_multiscale_2015()\n",
    "atlas_filename = dataset.scale444\n",
    "masker = input_data.NiftiLabelsMasker(labels_img=atlas_filename,high_pass=0.01 ,standardize=True,t_r=1.49,smoothing_fwhm=8,memory='nilearn_cache')\n",
    "masker.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame data loaded\n",
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(h5_filename,'r')\n",
    "data = f['state']\n",
    "label = f['action']\n",
    "sess = f['session']\n",
    "run = f['run']\n",
    "data_all = np.array(data)\n",
    "label_all = np.array(label)\n",
    "sess_all = np.array(sess)\n",
    "run_all = np.array(run)\n",
    "main_dataset = Concatdataset(data_all,label_all,sess_all,run_all)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('frame data loaded')\n",
    "\n",
    "model = get_models(model_num,sequence_len)\n",
    "model = model.to(device)\n",
    "if load_model:\n",
    "    saved_state = torch.load(LOAD_PATH ,map_location=device)\n",
    "    model.load_state_dict(saved_state)\n",
    "    print('Loaded model',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_5(model,main_dataset):\n",
    "    if True:\n",
    "    \n",
    "        test_loader = DataLoader(dataset=main_dataset, shuffle=False, batch_size=1)\n",
    "\n",
    " \n",
    "        allinput =[]\n",
    "        sess_all = []\n",
    "        run_all = []\n",
    "\n",
    "        if True:\n",
    "            test_accuracy=0.0\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batchid,traindata in enumerate(test_loader):\n",
    "                    data,labels,sess,run = traindata\n",
    "                    \n",
    "                    labels = labels.reshape(-1).to(device=device)\n",
    "                   \n",
    "                    data = data.float().to(device=device)\n",
    "                    \n",
    "                    inputdata = data.detach().cpu().numpy()\n",
    "                                     \n",
    "                    \n",
    "                    inputdata = inputdata[0].reshape(sequence_len,-1)\n",
    "                    allinput.append(inputdata)\n",
    "                    sess_all.append(sess)\n",
    "                    run_all.append(run)\n",
    "             \n",
    "            print(\"Accuracy: %.3f\"%(test_accuracy))\n",
    "            all_dict = {'inputdata':allinput,'session':sess_all,'run':run_all}\n",
    "            return all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_6(dl_activations):\n",
    "    if True:\n",
    "\n",
    "        inputdata = np.array(dl_activations['inputdata']) \n",
    "        session = np.array(dl_activations['session'])\n",
    "        run = np.array(dl_activations['run'])\n",
    "\n",
    "        with open(runshape_file_path, 'rb') as f:\n",
    "            test_shapes = pickle.load(f)\n",
    "        dl_all = np.arange(inputdata.shape[0])\n",
    "        \n",
    "        #split into runs\n",
    "        dl_final = []\n",
    "        start_idx = 0\n",
    "        stop_idx = 0\n",
    "        for i in range(len(test_shapes)):\n",
    "            start_idx = stop_idx\n",
    "            stop_idx = start_idx+test_shapes[i]\n",
    "            dl_final.append(dl_all[start_idx:stop_idx])\n",
    "        \n",
    "        tsum = 0\n",
    "        for i in range(len(dl_final)):\n",
    "            tsum = tsum + dl_final[i].shape[0]\n",
    "        #shape match check\n",
    "        if not(sum(test_shapes)==dl_all.shape[0]==tsum):\n",
    "            print('not pass 1')\n",
    "        \n",
    "        with open(fmri_file_path, 'rb') as f:\n",
    "            fmri_all = pickle.load(f)\n",
    "        df = pd.read_pickle(df_filepath_stage1)\n",
    "        \n",
    "        fmri_all2 = []\n",
    "        for i in range(len(df)):\n",
    "            onset_index = int(df['onset'][i]/1.49)\n",
    "            fmri_all2.append(fmri_all[i][onset_index:])\n",
    "        for i in range(len(fmri_all2)):\n",
    "            if(fmri_all2[i].shape[0]*1.49<df['duration'][i]):\n",
    "                print('not pass 2',i)\n",
    "                \n",
    "        fmri_all3 = []\n",
    "        for i in range(len(df)):\n",
    "            stop_index = int(df['duration'][i]/1.49)\n",
    "            fmri_all3.append(fmri_all2[i][:stop_index]) \n",
    "        for i in range(len(fmri_all3)):\n",
    "            diff = fmri_all3[i].shape[0]*1.49 - df['duration'][i]\n",
    "            if not(0>diff and diff>-1.49):\n",
    "                print('Not pass 3',i,diff)\n",
    "          \n",
    "        for i in range(len(df)):\n",
    "            if(fmri_all3[i].shape[0]-dl_final[i].shape[0]<0):\n",
    "                print('not pass 4',i)\n",
    "          \n",
    "                \n",
    "        #the dl is always small above\n",
    "        for i in range(len(df)):\n",
    "            fmri_all3[i]= fmri_all3[i][:dl_final[i].shape[0]]\n",
    "        for i in range(len(df)):\n",
    "            if not(fmri_all3[i].shape[0]==dl_final[i].shape[0]):\n",
    "                print('not pass 5',i)\n",
    "\n",
    "        lag_range = [4] # each lag = 1.5 secs\n",
    "        # find out common len for all lags\n",
    "        # largest lag will have shortest array size\n",
    "        common_length = np.zeros(len(df))\n",
    "        for i in range(len(df)):\n",
    "            common_length[i] = fmri_all3[i].shape[0]-max(lag_range)\n",
    "\n",
    "        for lag in lag_range:\n",
    "            #below note dl_final deals with indices\n",
    "            dl_final_all = []\n",
    "            fmri_final_all = []\n",
    "            for i in range(len(df)):\n",
    "                \n",
    "                dl_final_all.append(dl_final[i][:(fmri_all3[i].shape[0]-lag)])\n",
    "                fmri_final_all.append(fmri_all3[i][lag:])    \n",
    "\n",
    "            #for same size vectors for all considered lags\n",
    "            for i in range(len(df)):\n",
    "                dl_final_all[i] = dl_final_all[i][int(-common_length[i]):]\n",
    "                fmri_final_all[i] = fmri_final_all[i][int(-common_length[i]):]\n",
    "                \n",
    "            for i in range(len(df)):\n",
    "                if not(dl_final_all[i].shape[0]==fmri_final_all[i].shape[0]):\n",
    "                    print('not pass 6',i)\n",
    "                    \n",
    "            for i in range(len(df)):\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(fmri_final_all[i])\n",
    "                fmri_final_all[i] = scaler.transform(fmri_final_all[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "            new_dl_final = []\n",
    "            new_fmri_final = []\n",
    "            for i in range(len(df)):\n",
    "                if i not in exclude_runs:\n",
    "                    new_dl_final.append(dl_final_all[i])\n",
    "                    new_fmri_final.append(fmri_final_all[i])\n",
    "                \n",
    "            \n",
    "            \n",
    "            dl = np.concatenate(new_dl_final[:],axis=0)\n",
    "            fmri = np.concatenate(new_fmri_final[:],axis=0)\n",
    "            \n",
    "            tinput = inputdata[dl]\n",
    "            tsession = session[dl]\n",
    "            trun = run[dl]\n",
    "\n",
    "        all_dict = {'inputdata':tinput,'fmri':fmri,'session':tsession,'run':trun}\n",
    "        return all_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "dl_activations = stage_5(model,main_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not pass 4 0\n",
      "not pass 5 0\n"
     ]
    }
   ],
   "source": [
    "dl_fmri = stage_6(dl_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9664241945563473\n",
      "0.90584487\n"
     ]
    }
   ],
   "source": [
    "inputdata = dl_fmri['inputdata']\n",
    "fmri = dl_fmri['fmri']\n",
    "\n",
    "\n",
    "def transform_layer_activations2(layer,pca1):\n",
    "    layer = layer.reshape((-1,layer.shape[-1]))\n",
    "    pca = PCA(n_components=pca1)\n",
    "    pca.fit(layer)\n",
    "    print(pca.explained_variance_ratio_.sum())\n",
    "    layer = pca.transform(layer)\n",
    "    layer = layer.reshape((-1,sequence_len,layer.shape[-1]))\n",
    "    layer = layer.reshape(layer.shape[0],-1)\n",
    "    pca = PCA(n_components=800)\n",
    "    pca.fit(layer)\n",
    "    print(pca.explained_variance_ratio_.sum())\n",
    "    layer = pca.transform(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "inputdata = transform_layer_activations2(inputdata,800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess num of runs: 4\n",
      "[3 0 0 0]\n",
      "[15  0  0  0]\n",
      "[47  1  0  0]\n",
      "[55  8  0  0]\n",
      "sess num of runs: 4\n",
      "[7 0 0 0]\n",
      "[0 0 0 0]\n",
      "[24  1  0  0]\n",
      "[39  5  0  0]\n",
      "sess num of runs: 4\n",
      "[5 0 0 0]\n",
      "[36  1  0  0]\n",
      "[28  1  0  0]\n",
      "[17  0  0  0]\n",
      "sess num of runs: 4\n",
      "[10  0  0  0]\n",
      "[83  4  0  0]\n",
      "[45  2  0  0]\n",
      "[0 0 0 0]\n",
      "sess num of runs: 4\n",
      "[3 0 0 0]\n",
      "[42  0  0  0]\n",
      "[57  4  0  0]\n",
      "[9 0 0 0]\n",
      "sess num of runs: 5\n",
      "[45  5  0  0]\n",
      "[62 12  0  0]\n",
      "[34  0  0  0]\n",
      "[29  1  0  0]\n",
      "[5 0 0 0]\n",
      "sess num of runs: 4\n",
      "[19  2  0  0]\n",
      "[18  0  0  0]\n",
      "[8 1 0 0]\n",
      "[15  1  0  0]\n",
      "sess num of runs: 5\n",
      "[1 0 0 0]\n",
      "[44  1  0  0]\n",
      "[5 0 0 0]\n",
      "[11  0  0  0]\n",
      "[22  0  0  0]\n",
      "sess num of runs: 4\n",
      "[33  0  0  0]\n",
      "[32  3  0  0]\n",
      "[29  0  0  0]\n",
      "[23  0  0  0]\n",
      "sess num of runs: 5\n",
      "[27  1  0  0]\n",
      "[22  1  0  0]\n",
      "[32  6  0  0]\n",
      "[0 0 0 0]\n",
      "[29  1  0  0]\n",
      "-----------\n",
      "[8 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "dl_fin_temp = inputdata\n",
    "dl_fin = dl_fin_temp\n",
    "alpha = 0.2\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "\n",
    "all_test=[]\n",
    "for train_index, test_index in logo.split(dl_fin, fmri, dl_fmri['session']):\n",
    "\n",
    "    clf = Ridge(alpha=alpha,normalize=True)\n",
    "\n",
    "    clf.fit(dl_fin[train_index], fmri[train_index])\n",
    "    train_predict = clf.predict(dl_fin[train_index])\n",
    "    train_true = fmri[train_index]\n",
    "    r2_train = r2_score(train_true,train_predict,multioutput='raw_values').clip(min=0)\n",
    "\n",
    "    logo2 =  LeaveOneGroupOut()\n",
    "    print(\"sess num of runs:\",logo2.get_n_splits(dl_fin[test_index],fmri[test_index], dl_fmri['run'][test_index]))\n",
    "    \n",
    "    for run_train_index, run_test_index in logo2.split(dl_fin[test_index],fmri[test_index], dl_fmri['run'][test_index]):\n",
    "        test_predict = clf.predict(dl_fin[test_index[run_test_index]])\n",
    "        test_true = fmri[test_index[run_test_index]]\n",
    "        r2_test = r2_score(test_true,test_predict,multioutput='raw_values').clip(min=0)\n",
    "        all_test.append(r2_test)\n",
    "        print(np.histogram(r2_test, bins=[0.1, 0.2, 0.3,0.4,0.5])[0])\n",
    "            \n",
    "all_test = np.array(all_test[:])\n",
    "mean_r2  = all_test.mean(axis=0)\n",
    "            \n",
    "print('-----------')\n",
    "print(np.histogram(mean_r2, bins=[0.1, 0.2, 0.3,0.4,0.5])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('results/main-'+str(subject)+'-3.npy',all_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
